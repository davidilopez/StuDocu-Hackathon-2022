{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting en-core-web-sm==3.3.0\n",
                        "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
                        "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from en-core-web-sm==3.3.0) (3.3.0)\n",
                        "Requirement already satisfied: setuptools in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (62.3.2)\n",
                        "Requirement already satisfied: pathy>=0.3.5 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
                        "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
                        "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.16)\n",
                        "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
                        "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
                        "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
                        "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
                        "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
                        "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
                        "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
                        "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
                        "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
                        "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
                        "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.27.1)\n",
                        "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
                        "Requirement already satisfied: numpy>=1.15.0 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.22.4)\n",
                        "Requirement already satisfied: jinja2 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.1.2)\n",
                        "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
                        "Requirement already satisfied: packaging>=20.0 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
                        "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
                        "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.2.0)\n",
                        "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.12)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.5.18.1)\n",
                        "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.9)\n",
                        "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.1.3)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.1)\n",
                        "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
                        "You should consider upgrading via the '/home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
                        "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
                        "You can now load the package via spacy.load('en_core_web_sm')\n"
                    ]
                }
            ],
            "source": [
                "# !pip install gensim\n",
                "# !pip install git+https://github.com/boudinfl/pke.git\n",
                "!python -m spacy download en_core_web_sm\n",
                "# !pip install bert-extractive-summarizer --upgrade --force-reinstall\n",
                "# !pip install spacy==2.1.3 --upgrade --force-reinstall\n",
                "# !pip install -U nltk\n",
                "# !pip install -U pywsd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n",
                        "[nltk_data] Downloading collection 'popular'\n",
                        "[nltk_data]    | \n",
                        "[nltk_data]    | Downloading package cmudict to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package cmudict is already up-to-date!\n",
                        "[nltk_data]    | Downloading package gazetteers to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
                        "[nltk_data]    | Downloading package genesis to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package genesis is already up-to-date!\n",
                        "[nltk_data]    | Downloading package gutenberg to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
                        "[nltk_data]    | Downloading package inaugural to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package inaugural is already up-to-date!\n",
                        "[nltk_data]    | Downloading package movie_reviews to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
                        "[nltk_data]    | Downloading package names to /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package names is already up-to-date!\n",
                        "[nltk_data]    | Downloading package shakespeare to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
                        "[nltk_data]    | Downloading package stopwords to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package stopwords is already up-to-date!\n",
                        "[nltk_data]    | Downloading package treebank to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package treebank is already up-to-date!\n",
                        "[nltk_data]    | Downloading package twitter_samples to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
                        "[nltk_data]    | Downloading package omw to /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package omw is already up-to-date!\n",
                        "[nltk_data]    | Downloading package omw-1.4 to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
                        "[nltk_data]    | Downloading package wordnet to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package wordnet is already up-to-date!\n",
                        "[nltk_data]    | Downloading package wordnet2021 to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
                        "[nltk_data]    | Downloading package wordnet31 to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
                        "[nltk_data]    | Downloading package wordnet_ic to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
                        "[nltk_data]    | Downloading package words to /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package words is already up-to-date!\n",
                        "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
                        "[nltk_data]    | Downloading package punkt to /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package punkt is already up-to-date!\n",
                        "[nltk_data]    | Downloading package snowball_data to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
                        "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
                        "[nltk_data]    |     /home/david/nltk_data...\n",
                        "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
                        "[nltk_data]    |       to-date!\n",
                        "[nltk_data]    | \n",
                        "[nltk_data]  Done downloading collection popular\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import nltk\n",
                "\n",
                "nltk.download(\"stopwords\")\n",
                "nltk.download(\"popular\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "\n",
                "\n",
                "\n",
                "## BERT Extractive Summarizer\n",
                "Summarize the text using BERT extractive summarizer. This is used to find important sentences and useful sentences from the complete text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/david/.local/share/virtualenvs/MCQ-StuDocu-Hackathon-2022-9n7-6BLn/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n",
                        "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
                        "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
                        "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
                    ]
                }
            ],
            "source": [
                "from summarizer import Summarizer\n",
                "\n",
                "with open(\"egypt.txt\", \"r\") as f:\n",
                "    full_text = f.read()\n",
                "\n",
                "model = Summarizer()\n",
                "result = model(full_text, min_length=60, max_length=500, ratio=0.4)\n",
                "\n",
                "summarized_text = \"\".join(result)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Keyword Extraction\n",
                "Get important keywords from the text and filter those keywords that are present in the summarized text."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "import itertools\n",
                "import re\n",
                "import pke\n",
                "import string\n",
                "from nltk.corpus import stopwords\n",
                "\n",
                "\n",
                "def get_nouns_multipartite(text):\n",
                "    extractor = pke.unsupervised.MultipartiteRank()\n",
                "    extractor.load_document(input=text)\n",
                "    #    not contain punctuation marks or stopwords as candidates.\n",
                "    pos = {\"PROPN\"}\n",
                "    # pos = {'VERB', 'ADJ', 'NOUN'}\n",
                "    stoplist = list(string.punctuation)\n",
                "    stoplist += [\"-lrb-\", \"-rrb-\", \"-lcb-\", \"-rcb-\", \"-lsb-\", \"-rsb-\"]\n",
                "    stoplist += stopwords.words(\"english\")\n",
                "    # extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
                "    extractor.candidate_selection(pos=pos)\n",
                "    # 4. build the Multipartite graph and rank candidates using random walk,\n",
                "    #    alpha controls the weight adjustment mechanism, see TopicRank for\n",
                "    #    threshold/method parameters.\n",
                "    extractor.candidate_weighting(alpha=1.1, threshold=0.75, method=\"average\")\n",
                "    keyphrases = extractor.get_n_best(n=20)\n",
                "\n",
                "    return [key[0] for key in keyphrases]\n",
                "\n",
                "\n",
                "keywords = get_nouns_multipartite(full_text)\n",
                "filtered_keys = [\n",
                "    keyword for keyword in keywords if keyword.lower() in summarized_text.lower()\n",
                "]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Sentence Mapping\n",
                "For each keyword get the sentences from the summarized text containing that keyword. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "from nltk.tokenize import sent_tokenize\n",
                "from flashtext import KeywordProcessor\n",
                "\n",
                "\n",
                "def tokenize_sentences(text):\n",
                "    sentences = [sent_tokenize(text)]\n",
                "    sentences = [y for x in sentences for y in x]\n",
                "    # Remove any short sentences less than 20 letters.\n",
                "    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]\n",
                "    return sentences\n",
                "\n",
                "\n",
                "def get_sentences_for_keyword(keywords, sentences):\n",
                "    keyword_processor = KeywordProcessor()\n",
                "    keyword_sentences = {}\n",
                "    for word in keywords:\n",
                "        keyword_sentences[word] = []\n",
                "        keyword_processor.add_keyword(word)\n",
                "    for sentence in sentences:\n",
                "        keywords_found = keyword_processor.extract_keywords(sentence)\n",
                "        for key in keywords_found:\n",
                "            keyword_sentences[key].append(sentence)\n",
                "\n",
                "    for key, values in keyword_sentences.items():\n",
                "        values = sorted(values, key=len, reverse=True)\n",
                "        keyword_sentences[key] = values\n",
                "    return keyword_sentences\n",
                "\n",
                "\n",
                "sentences = tokenize_sentences(summarized_text)\n",
                "keyword_sentence_mapping = get_sentences_for_keyword(filtered_keys, sentences)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Generate MCQ\n",
                "Get distractors (wrong answer choices) from Wordnet/Conceptnet and generate MCQ Questions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "import json\n",
                "import re\n",
                "import random\n",
                "from nltk.corpus import wordnet as wn\n",
                "from pywsd.similarity import max_similarity\n",
                "from pywsd.lesk import adapted_lesk\n",
                "from pywsd.lesk import simple_lesk\n",
                "from pywsd.lesk import cosine_lesk\n",
                "import conceptnet_lite\n",
                "from conceptnet_lite import Label, edges_for\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'conceptnet_lite' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "\u001b[1;32m/home/david/Documents/Git/MCQ-StuDocu-Hackathon-2022/notebooks/mcq_generator.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/david/Documents/Git/MCQ-StuDocu-Hackathon-2022/notebooks/mcq_generator.ipynb#ch0000013?line=0'>1</a>\u001b[0m conceptnet_lite\u001b[39m.\u001b[39mconnect()\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'conceptnet_lite' is not defined"
                    ]
                }
            ],
            "source": [
                "conceptnet_lite.connect()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "#############################################################################\n",
                        "NOTE::::::::  Since the algorithm might have errors along the way, wrong answer choices generated might not be correct for some questions. \n",
                        "#############################################################################\n",
                        "\n",
                        "\n",
                        "1)  _______  cut the stems into strips, pressed them, and dried them into sheets that could be rolled into scrolls.\n",
                        "\t a )   Bantu\n",
                        "\t b )   Angolan\n",
                        "\t c )   Egyptians\n",
                        "\t d )   Algerian\n",
                        "\n",
                        "More options:  ['Basotho', 'Beninese', 'Berber', 'Black African', 'Burundian', 'Cameroonian', 'Carthaginian', 'Chadian', 'Chewa', 'Congolese', 'Djiboutian', 'Egyptian', 'Ethiopian', 'Eurafrican', 'Ewe', 'Fulani'] \n",
                        "\n",
                        "\n",
                        "2) It combined the red  _______  of Lower Egypt with the white  _______  of Upper Egypt.\n",
                        "\t a )   Academic Degree\n",
                        "\t b )   Aliyah\n",
                        "\t c )   Crown\n",
                        "\t d )   Academy Award\n",
                        "\n",
                        "More options:  ['Cachet', 'Citation', 'Decoration', 'Emmy', 'Letter', 'Mention', 'Nobel Prize', 'Pennant', 'Prix De Rome', 'Prix Goncourt', 'Trophy'] \n",
                        "\n",
                        "\n",
                        "3)  _______  says a king named Narmer united Upper and Lower Egypt.\n",
                        "\t a )   Love Story\n",
                        "\t b )   Adventure Story\n",
                        "\t c )   Legend\n",
                        "\t d )   Fable\n",
                        "\n",
                        "More options:  ['Mystery', 'Myth', 'Parable', 'Plot', 'Short Story'] \n",
                        "\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Distractors from Wordnet\n",
                "def get_distractors_wordnet(syn, word):\n",
                "    distractors = []\n",
                "    word = word.lower()\n",
                "    orig_word = word\n",
                "    if len(word.split()) > 0:\n",
                "        word = word.replace(\" \", \"_\")\n",
                "    hypernym = syn.hypernyms()\n",
                "    if len(hypernym) == 0:\n",
                "        return distractors\n",
                "    for item in hypernym[0].hyponyms():\n",
                "        name = item.lemmas()[0].name()\n",
                "        if name == orig_word:\n",
                "            continue\n",
                "        name = name.replace(\"_\", \" \")\n",
                "        name = \" \".join(w.capitalize() for w in name.split())\n",
                "        if name is not None and name not in distractors:\n",
                "            distractors.append(name)\n",
                "    return distractors\n",
                "\n",
                "\n",
                "def get_wordsense(sent, word):\n",
                "    word = word.lower()\n",
                "\n",
                "    if len(word.split()) > 0:\n",
                "        word = word.replace(\" \", \"_\")\n",
                "\n",
                "    if synsets := wn.synsets(word, \"n\"):\n",
                "        wup = max_similarity(sent, word, \"wup\", pos=\"n\")\n",
                "        adapted_lesk_output = adapted_lesk(sent, word, pos=\"n\")\n",
                "        lowest_index = min(synsets.index(wup), synsets.index(adapted_lesk_output))\n",
                "        return synsets[lowest_index]\n",
                "    else:\n",
                "        return None\n",
                "\n",
                "\n",
                "# Distractors from http://conceptnet.io/\n",
                "def get_distractors_conceptnet(word):\n",
                "    word = word.lower()\n",
                "    original_word = word\n",
                "    if len(word.split()) > 0:\n",
                "        word = word.replace(\" \", \"_\")\n",
                "    distractor_list = []\n",
                "\n",
                "    try:\n",
                "        obj = Label.get(text=word, language=\"en\").concepts\n",
                "        edges = edges_for(obj, same_language=True)\n",
                "    except Exception:\n",
                "        return distractor_list\n",
                "    for edge in edges:\n",
                "        try:\n",
                "            link = edge.end.text\n",
                "            obj2 = Label.get(text=link, language=\"en\").concepts\n",
                "            edges2 = edges_for(obj2, same_language=True)\n",
                "            for edge in edges2:\n",
                "                if edge.relation.name in [\n",
                "                    \"PartOf\",\n",
                "                    \"SimilarTo\",\n",
                "                    \"DistinctFrom\",\n",
                "                    \"MannerOf\",\n",
                "                ]:\n",
                "                    word2 = edge.start.text\n",
                "                    if (\n",
                "                        word2 not in distractor_list\n",
                "                        and original_word.lower() != word2.lower()\n",
                "                    ):\n",
                "                        distractor_list.append(word2.replace(\"_\", \" \"))\n",
                "        except Exception:\n",
                "            continue\n",
                "\n",
                "    return distractor_list\n",
                "\n",
                "\n",
                "key_distractor_list = {}\n",
                "\n",
                "for keyword in keyword_sentence_mapping:\n",
                "    if wordsense := get_wordsense(keyword_sentence_mapping[keyword][0], keyword):\n",
                "        distractors = get_distractors_wordnet(wordsense, keyword)\n",
                "        if len(distractors) == 0:\n",
                "            distractors = get_distractors_conceptnet(keyword)\n",
                "    else:\n",
                "        distractors = get_distractors_conceptnet(keyword)\n",
                "    if len(distractors) != 0:\n",
                "        key_distractor_list[keyword] = distractors\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'question_set': [{'question': ' _______  cut the stems into strips, pressed them, and dried them into sheets that could be rolled into scrolls.', 'choices': ['Egyptians', 'Angolan', 'Bantu', 'Algerian'], 'answer': 'egyptians'}, {'question': 'It combined the red  _______  of Lower Egypt with the white  _______  of Upper Egypt.', 'choices': ['Aliyah', 'Academic Degree', 'Academy Award', 'Crown'], 'answer': 'crown'}, {'question': ' _______  says a king named Narmer united Upper and Lower Egypt.', 'choices': ['Fable', 'Love Story', 'Legend', 'Adventure Story'], 'answer': 'legend'}]}\n"
                    ]
                }
            ],
            "source": [
                "final_output = {\"question_set\": []}\n",
                "for each in key_distractor_list:\n",
                "    sentence = keyword_sentence_mapping[each][0]\n",
                "    pattern = re.compile(each, re.IGNORECASE)\n",
                "    output = pattern.sub(\" _______ \", sentence)\n",
                "    choices = [each.capitalize()] + key_distractor_list[each]\n",
                "    top4choices = choices[:4]\n",
                "    random.shuffle(top4choices)\n",
                "    final_output[\"question_set\"].append({\"question\": output, \"choices\": top4choices, \"answer\": each})\n"
            ]
        }
    ],
    "metadata": {
        "interpreter": {
            "hash": "1bea3ce7022433a54249e992e857614ec9cb1a23126accc446915e82ac3a0a3e"
        },
        "kernelspec": {
            "display_name": "Python 3.8.10 ('MCQ-StuDocu-Hackathon-2022-9n7-6BLn')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
